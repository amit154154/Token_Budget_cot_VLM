{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Train using only the text"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1cbb634d10aed818"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    get_linear_schedule_with_warmup\n",
    ")\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) Login to Hugging Face Hub and load your dataset\n",
    "# -------------------------------------------------------------------\n",
    "login(\"my_hf_key\")\n",
    "dataset = load_dataset(\"5CD-AI/LLaVA-CoT-o1-Instruct\")  # check what splits are available\n",
    "print(dataset)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) A simple dataset class\n",
    "# -------------------------------------------------------------------\n",
    "class RationaleTokenCountDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        A dataset that takes a Hugging Face dataset split and\n",
    "        creates samples (input_ids, attention_mask, labels).\n",
    "        \"\"\"\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        if tokenizer.pad_token is None:\n",
    "            self.tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        import re\n",
    "        item = self.hf_dataset[idx]\n",
    "        \n",
    "        # Adjust these keys to match your actual dataset fields:\n",
    "        question_text = item[\"question\"]  # might be \"instruction\" or something else\n",
    "        output_text   = item[\"output\"]    # The field containing <REASONING>...</REASONING>\n",
    "\n",
    "        # Extract rationale from <REASONING>...</REASONING>\n",
    "        match = re.search(r\"<REASONING>(.*?)</REASONING>\", output_text, re.DOTALL)\n",
    "        rationale_text = match.group(1) if match else \"\"\n",
    "\n",
    "        # Tokenize rationale to count its length\n",
    "        rationale_tokens = self.tokenizer.tokenize(rationale_text)\n",
    "        label = len(rationale_tokens)\n",
    "\n",
    "        # Tokenize the question\n",
    "        encoding = self.tokenizer(\n",
    "            question_text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) The Model class without 'total_steps'\n",
    "# -------------------------------------------------------------------\n",
    "class RationaleLengthRegressor(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"HuggingFaceTB/SmolLM2-135M\",\n",
    "        lr: float = 1e-4,\n",
    "        warmup_steps: int = 1000,\n",
    "        tokenizer=None,\n",
    "        print_every: int = 50\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A PyTorch Lightning module that fine-tunes (via LoRA) a GPT-like model\n",
    "        to predict log(1 + length). We keep a fixed 1000-step warmup,\n",
    "        and remove the 'total_steps' from init; we will set total steps externally.\n",
    "\n",
    "        Args:\n",
    "            model_name:    The name/path of the pretrained model.\n",
    "            lr:            Learning rate.\n",
    "            warmup_steps:  Number of warmup steps (we fix it at 1000).\n",
    "            tokenizer:     A Hugging Face tokenizer for debugging prints.\n",
    "            print_every:   Print debug info every X steps.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"tokenizer\"])\n",
    "\n",
    "        self.lr = lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.tokenizer = tokenizer\n",
    "        self.print_every = print_every\n",
    "\n",
    "        # This will be computed later and set via set_total_training_steps()\n",
    "        self.total_training_steps = None\n",
    "\n",
    "        # 1) Load a pretrained causal LM as the backbone\n",
    "        self.backbone = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "        # 2) Define LoRA configuration\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "        )\n",
    "\n",
    "        # 3) Wrap the backbone with LoRA\n",
    "        self.backbone = get_peft_model(self.backbone, lora_config)\n",
    "\n",
    "        # 4) A deeper regression head\n",
    "        hidden_size = self.backbone.config.hidden_size\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(hidden_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 1)  # predicting log(1 + length)\n",
    "        )\n",
    "\n",
    "    def set_total_training_steps(self, total_steps: int):\n",
    "        \"\"\"\n",
    "        Set the total number of training steps (calculated externally).\n",
    "        \"\"\"\n",
    "        self.total_training_steps = total_steps\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        last_hidden_state = outputs.hidden_states[-1]  # final hidden layer\n",
    "        pooled = last_hidden_state[:, 0, :]\n",
    "        return self.regression_head(pooled).squeeze(-1)  # log(1 + length)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"].float()\n",
    "\n",
    "        # Convert labels to log-space\n",
    "        labels_log = torch.log1p(labels)\n",
    "\n",
    "        preds_log = self(input_ids, attention_mask)\n",
    "        loss = F.mse_loss(preds_log, labels_log)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "\n",
    "        # Debug info\n",
    "        if (batch_idx % self.print_every == 0) and (self.tokenizer is not None):\n",
    "            decoded_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            real_len = labels[0].item()\n",
    "            predicted_len = torch.expm1(preds_log[0]).item()  # invert\n",
    "            print(f\"--- Step {batch_idx} Debug Info ---\")\n",
    "            print(f\"Decoded Input: {decoded_text}\")\n",
    "            print(f\"Real Length: {real_len:.2f}\")\n",
    "            print(f\"Predicted Length: {predicted_len:.2f}\")\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"].float()\n",
    "\n",
    "        labels_log = torch.log1p(labels)\n",
    "        preds_log = self(input_ids, attention_mask)\n",
    "        loss = F.mse_loss(preds_log, labels_log)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        Setup optimizer and linear LR schedule with warmup.\n",
    "        We'll create the scheduler only after we've set total_training_steps.\n",
    "        \"\"\"\n",
    "        if self.total_training_steps is None:\n",
    "            raise ValueError(\n",
    "                \"total_training_steps has not been set yet. \"\n",
    "                \"Call `model.set_total_training_steps(...)` before trainer.fit(...)\"\n",
    "            )\n",
    "\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "\n",
    "        # Create linear schedule with warmup\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.warmup_steps,\n",
    "            num_training_steps=self.total_training_steps\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n",
    "\n",
    "\n",
    "def main():\n",
    "    from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "    batch_size = 64\n",
    "\n",
    "    # 1) We assume your dataset has a \"train\" split. \n",
    "    #    If it has other splits, adjust accordingly.\n",
    "    full_dataset = dataset[\"train\"]\n",
    "    print(\"Number of samples in 'train':\", len(full_dataset))\n",
    "\n",
    "    # 2) Train-val split (80%/20%)\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    dataset_train, dataset_val = random_split(full_dataset, [train_size, val_size])\n",
    "    print(f\"Train size: {len(dataset_train)} | Val size: {len(dataset_val)}\")\n",
    "\n",
    "    # 3) Init tokenizer\n",
    "    checkpoint = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "    # 4) Create our custom datasets\n",
    "    train_dataset = RationaleTokenCountDataset(dataset_train, tokenizer, max_length=128)\n",
    "    val_dataset   = RationaleTokenCountDataset(dataset_val,   tokenizer, max_length=128)\n",
    "\n",
    "    # 5) Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=3)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=3)\n",
    "\n",
    "    # 6) Create model (no total_steps argument anymore)\n",
    "    model = RationaleLengthRegressor(\n",
    "        model_name=checkpoint,\n",
    "        tokenizer=tokenizer,\n",
    "        lr=1e-4,\n",
    "        warmup_steps=50,  # fixed 1000 warmup steps\n",
    "        print_every=50\n",
    "    )\n",
    "\n",
    "    # 7) Calculate total training steps = steps_per_epoch * max_epochs\n",
    "    #    We'll train for 5 epochs\n",
    "    max_epochs = 20\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    total_training_steps = steps_per_epoch * max_epochs\n",
    "\n",
    "    # 8) Set the total training steps on the model\n",
    "    model.set_total_training_steps(total_training_steps)\n",
    "\n",
    "    # 9) Setup a Weights & Biases logger\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"my-rationale-length-project\",\n",
    "        name=\"smolLM2-regression-run\",\n",
    "        log_model=True\n",
    "    )\n",
    "\n",
    "    # Optionally, add a checkpoint callback to save your model\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "        dirpath=\"./checkpoints\",\n",
    "        filename=\"best-checkpoint\"\n",
    "    )\n",
    "\n",
    "    # 10) Create a PyTorch Lightning Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator=\"auto\",\n",
    "        devices=\"auto\",\n",
    "        precision=\"16-mixed\",\n",
    "        log_every_n_steps=50,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=[checkpoint_callback]\n",
    "    )\n",
    "\n",
    "    # 11) Train\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train using the text+image"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9953c5ffcbf9b382"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import requests\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from datasets import load_dataset\n",
    "from huggingface_hub import login\n",
    "\n",
    "# For logging\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "# Hugging Face Transformers / PEFT\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    SiglipVisionModel,\n",
    ")\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 1) A custom dataset that returns text + labels + a *transformed* image tensor\n",
    "# -------------------------------------------------------------------\n",
    "class RationaleTokenCountDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, hf_dataset, tokenizer, max_length=128):\n",
    "        \"\"\"\n",
    "        A dataset that takes a Hugging Face dataset split and\n",
    "        creates samples (input_ids, attention_mask, labels, image_tensor).\n",
    "\n",
    "        Expected fields in hf_dataset[idx]:\n",
    "            - \"question\" : str\n",
    "            - \"output\"   : str (that has <REASONING>...</REASONING>)\n",
    "            - \"image\"    : PIL.Image or something convertible to PIL.Image\n",
    "        \"\"\"\n",
    "        self.hf_dataset = hf_dataset\n",
    "        self.tokenizer = tokenizer\n",
    "        if tokenizer.pad_token is None:\n",
    "            self.tokenizer.add_special_tokens({\"pad_token\": tokenizer.eos_token})\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # According to the SigLIP processor config:\n",
    "        #   do_resize = True (224 x 224)\n",
    "        #   do_rescale = True (factor = 1/255)\n",
    "        #   do_normalize = True (mean=[0.5]*3, std=[0.5]*3)\n",
    "        # We'll replicate that in a torchvision transforms pipeline:\n",
    "        self.image_transform = T.Compose([\n",
    "            T.Resize((224, 224), interpolation=T.InterpolationMode.BICUBIC),\n",
    "            T.ToTensor(),  # => converts [0..255] to [0..1]\n",
    "            T.Normalize(mean=[0.5, 0.5, 0.5],\n",
    "                        std=[0.5, 0.5, 0.5])  # => transforms [0..1] into [-1..1]\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.hf_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.hf_dataset[idx]\n",
    "\n",
    "        question_text = item[\"question\"]\n",
    "        output_text   = item[\"output\"]\n",
    "        pil_image     = item[\"image\"]  # Should already be a PIL image\n",
    "\n",
    "        # Extract rationale from <REASONING>...</REASONING>\n",
    "        match = re.search(r\"<REASONING>(.*?)</REASONING>\", output_text, re.DOTALL)\n",
    "        rationale_text = match.group(1) if match else \"\"\n",
    "\n",
    "        # Tokenize rationale to determine the label (its token count)\n",
    "        rationale_tokens = self.tokenizer.tokenize(rationale_text)\n",
    "        label = len(rationale_tokens)\n",
    "\n",
    "        # Tokenize the question\n",
    "        encoding = self.tokenizer(\n",
    "            question_text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        # Transform the image into a tensor\n",
    "        image_tensor = self.image_transform(pil_image)  # shape: (3, 224, 224)\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(0),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long),\n",
    "            \"image\": image_tensor,  # Return the transformed tensor\n",
    "            \"question_text\": question_text,  # We'll log it\n",
    "        }\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 2) Frozen SigLIP Vision Encoder (no AutoProcessor usage anymore)\n",
    "# -------------------------------------------------------------------\n",
    "class SigLIPFrozenEncoder(nn.Module):\n",
    "    def __init__(self, model_name=\"google/siglip-base-patch16-224\"):\n",
    "        \"\"\"\n",
    "        Loads a SigLIP vision model and freezes it (no gradient updates).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.model = SiglipVisionModel.from_pretrained(model_name)\n",
    "\n",
    "        # The SigLIP model config usually has a 'hidden_size' attribute\n",
    "        self.output_dim = self.model.config.hidden_size\n",
    "\n",
    "        # Freeze all parameters (no gradient updates)\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, image_tensors: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_tensors: shape (B, 3, 224, 224), already preprocessed to [-1,1]\n",
    "\n",
    "        Returns:\n",
    "            outputs.pooler_output: shape (B, hidden_size)\n",
    "        \"\"\"\n",
    "        device = next(self.model.parameters()).device\n",
    "        image_tensors = image_tensors.to(device)  # Move to correct device\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # SiglipVisionModel expects keyword arg \"pixel_values\"\n",
    "            outputs = self.model(pixel_values=image_tensors)\n",
    "\n",
    "        return outputs.pooler_output  # shape (B, hidden_size)\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 3) PyTorch Lightning Module that predicts log(1 + length) from text+image\n",
    "# -------------------------------------------------------------------\n",
    "class RationaleLengthRegressor(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"HuggingFaceTB/SmolLM2-135M\",\n",
    "        image_model_name: str = \"google/siglip-base-patch16-224\",\n",
    "        lr: float = 1e-4,\n",
    "        warmup_steps: int = 1000,\n",
    "        tokenizer=None,\n",
    "        print_every: int = 50\n",
    "    ):\n",
    "        \"\"\"\n",
    "        A PyTorch Lightning module that:\n",
    "         - Fine-tunes (via LoRA) a GPT-like text model\n",
    "         - Freezes a SigLIP vision encoder\n",
    "         - Concatenates text_cls + image_cls to predict log(1 + rationale_length).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters(ignore=[\"tokenizer\"])\n",
    "\n",
    "        self.lr = lr\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.tokenizer = tokenizer\n",
    "        self.print_every = print_every\n",
    "\n",
    "        self.total_training_steps = None\n",
    "\n",
    "        # 1) Text backbone + LoRA\n",
    "        self.backbone = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        lora_config = LoraConfig(\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            target_modules=[\"q_proj\", \"v_proj\"],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=TaskType.CAUSAL_LM,\n",
    "        )\n",
    "        self.backbone = get_peft_model(self.backbone, lora_config)\n",
    "\n",
    "        # 2) Load & freeze the SigLIP vision encoder\n",
    "        self.image_encoder = SigLIPFrozenEncoder(model_name=image_model_name)\n",
    "\n",
    "        # 3) Create the regression head\n",
    "        text_hidden_size = self.backbone.config.hidden_size    # e.g., 768\n",
    "        img_hidden_size  = self.image_encoder.output_dim       # e.g., 768\n",
    "        combined_size = text_hidden_size + img_hidden_size\n",
    "\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(combined_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(128, 1)  # predicts log(1 + length)\n",
    "        )\n",
    "\n",
    "    def set_total_training_steps(self, total_steps: int):\n",
    "        self.total_training_steps = total_steps\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, images):\n",
    "        # 1) Text embeddings\n",
    "        text_outputs = self.backbone(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        last_hidden = text_outputs.hidden_states[-1]  # (B, T, text_hidden_size)\n",
    "        text_cls = last_hidden[:, 0, :]               # take token at index 0\n",
    "\n",
    "        # 2) Image embeddings (SigLIP is frozen, images are preprocessed)\n",
    "        img_cls = self.image_encoder(images)          # (B, img_hidden_size)\n",
    "\n",
    "        # 3) Combine\n",
    "        combined = torch.cat([text_cls, img_cls], dim=-1)\n",
    "        preds_log = self.regression_head(combined).squeeze(-1)\n",
    "        return preds_log\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"].float()\n",
    "        images = batch[\"image\"]  # shape (B, 3, 224, 224)\n",
    "        question_text = batch[\"question_text\"][0]  # first sample's question\n",
    "\n",
    "        # Convert labels to log-space\n",
    "        labels_log = torch.log1p(labels)\n",
    "\n",
    "        preds_log = self(input_ids, attention_mask, images)\n",
    "        loss = F.mse_loss(preds_log, labels_log)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "\n",
    "        # Optional debug info\n",
    "        if (batch_idx % self.print_every == 0) and (self.tokenizer is not None):\n",
    "            decoded_text = self.tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "            real_len = labels[0].item()\n",
    "            predicted_len = torch.expm1(preds_log[0]).item()  # invert\n",
    "\n",
    "            print(f\"\\n--- Step {batch_idx} Debug ---\")\n",
    "            print(f\"Question (first sample): {decoded_text}\")\n",
    "            print(f\"Real Len: {real_len:.2f}, Predicted: {predicted_len:.2f}\")\n",
    "\n",
    "            # =======================\n",
    "            #  Log the first image\n",
    "            # =======================\n",
    "            # Convert the first sample's image (normalized) back to a PIL Image\n",
    "            image_0 = images[0].clone().detach().cpu()\n",
    "            pil_image_0 = T.ToPILImage()(image_0)\n",
    "            caption = (\n",
    "                f\"Q: {question_text}\\n\"\n",
    "                f\"Real: {real_len:.2f}, Pred: {predicted_len:.2f}\"\n",
    "            )\n",
    "\n",
    "            # Use the experiment's log method to store the image in W&B\n",
    "            self.logger.experiment.log(\n",
    "                {\"debug_image\": wandb.Image(pil_image_0, caption=caption)}\n",
    "            )\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        attention_mask = batch[\"attention_mask\"]\n",
    "        labels = batch[\"labels\"].float()\n",
    "        images = batch[\"image\"]\n",
    "\n",
    "        labels_log = torch.log1p(labels)\n",
    "        preds_log = self(input_ids, attention_mask, images)\n",
    "        loss = F.mse_loss(preds_log, labels_log)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if self.total_training_steps is None:\n",
    "            raise ValueError(\n",
    "                \"total_training_steps has not been set yet. \"\n",
    "                \"Call `model.set_total_training_steps(...)` before trainer.fit(...)\"\n",
    "            )\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=self.warmup_steps,\n",
    "            num_training_steps=self.total_training_steps\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]\n",
    "\n",
    "\n",
    "# -------------------------------------------------------------------\n",
    "# 4) Main training script\n",
    "# -------------------------------------------------------------------\n",
    "def main():\n",
    "    # 1) (Optional) Login to Hugging Face if needed\n",
    "    # Replace \"YOUR_HF_TOKEN\" with your actual token if needed for private repos\n",
    "    # login(\"YOUR_HF_TOKEN\")\n",
    "\n",
    "    # 2) Load your dataset from HF (or local)\n",
    "    dataset = load_dataset(\"5CD-AI/LLaVA-CoT-o1-Instruct\")\n",
    "    full_dataset = dataset[\"train\"]\n",
    "    print(\"Number of samples in 'train':\", len(full_dataset))\n",
    "\n",
    "    # 3) Train-val split\n",
    "    train_size = int(0.9 * len(full_dataset))\n",
    "    val_size = len(full_dataset) - train_size\n",
    "    dataset_train, dataset_val = random_split(full_dataset, [train_size, val_size])\n",
    "    print(f\"Train size: {len(dataset_train)} | Val size: {len(dataset_val)}\")\n",
    "\n",
    "    # 4) Create tokenizer\n",
    "    text_checkpoint = \"HuggingFaceTB/SmolLM2-135M\"  # or \"facebook/opt-350m\", etc.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(text_checkpoint)\n",
    "\n",
    "    # 5) Create custom dataset objects\n",
    "    train_dataset = RationaleTokenCountDataset(dataset_train, tokenizer, max_length=128)\n",
    "    val_dataset   = RationaleTokenCountDataset(dataset_val,   tokenizer, max_length=128)\n",
    "\n",
    "    # 6) Create DataLoaders\n",
    "    batch_size = 32\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=3)\n",
    "    val_loader   = DataLoader(val_dataset,   batch_size=batch_size, shuffle=False, num_workers=3)\n",
    "\n",
    "    # 7) Create model\n",
    "    model = RationaleLengthRegressor(\n",
    "        model_name=text_checkpoint,\n",
    "        image_model_name=\"google/siglip-base-patch16-224\",  # SigLIP model\n",
    "        tokenizer=tokenizer,\n",
    "        lr=1e-4,\n",
    "        warmup_steps=1000,\n",
    "        print_every=50\n",
    "    )\n",
    "\n",
    "    # 8) Set total training steps\n",
    "    max_epochs = 15\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    total_training_steps = steps_per_epoch * max_epochs\n",
    "    model.set_total_training_steps(total_training_steps)\n",
    "\n",
    "    # 9) Setup W&B logger\n",
    "    wandb_logger = WandbLogger(\n",
    "        project=\"my-rationale-length-project\",\n",
    "        name=\"siglip-smolLM2-run\",\n",
    "        log_model=True\n",
    "    )\n",
    "\n",
    "    # 10) Setup checkpoint callback\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_loss\",\n",
    "        save_top_k=1,\n",
    "        mode=\"min\",\n",
    "        dirpath=\"./checkpoints\",\n",
    "        filename=\"best-checkpoint\"\n",
    "    )\n",
    "\n",
    "    # 11) Trainer\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=max_epochs,\n",
    "        accelerator=\"auto\",\n",
    "        devices=\"auto\",\n",
    "        precision=\"16-mixed\",\n",
    "        log_every_n_steps=50,\n",
    "        logger=wandb_logger,\n",
    "        callbacks=[checkpoint_callback]\n",
    "    )\n",
    "\n",
    "    # 12) Train!\n",
    "    trainer.fit(model, train_loader, val_loader)\n",
    "\n",
    "    # 13) Finish WandB\n",
    "    wandb.finish()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8ee20b09f0034921"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
